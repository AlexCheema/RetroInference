{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the model\n",
    "class GRUTextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(GRUTextGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_size)\n",
    "        output, hidden = self.gru(embedded, hidden)  # (batch_size, seq_length, hidden_size)\n",
    "        logits = self.fc(output)  # (batch_size, seq_length, vocab_size)\n",
    "        return logits, hidden\n",
    "\n",
    "token_dict = {letter:i for letter,i in zip(string.ascii_lowercase, range(1,27))}\n",
    "token_dict[' '] = 0\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sentences, idx):\n",
    "        self.data = []\n",
    "\n",
    "        self.max_len = max([len(x) for x in sentences])\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if len(sentence) > idx:\n",
    "                x = torch.tensor([token_dict[x] for x in sentence[:idx]], dtype=torch.int64, device=device)\n",
    "                y = torch.tensor(token_dict[sentence[idx]], dtype=torch.int64, device=device)\n",
    "                self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "# Training loop\n",
    "def train(model, dataloaders, vocab_size, device, epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data_loader in dataloaders:\n",
    "            total_loss = 0\n",
    "            for _, (inputs, targets) in enumerate(data_loader, 0):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                logits, _ = model(inputs)  # (batch_size, seq_length, vocab_size)\n",
    "                logits = logits[:, -1, :]\n",
    "                logits = logits.view(-1, vocab_size)  # Reshape for loss calculation\n",
    "                targets = targets.view(-1)  # Reshape to match logits\n",
    "                \n",
    "                loss = criterion(logits, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "# Example hyperparameters\n",
    "vocab_size = 27  # a-z and a space character\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "batch_size = 64\n",
    "seq_length = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model\n",
    "model = GRUTextGenerator(vocab_size, embed_size, hidden_size, num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Replace with your own DataLoader\n",
    "sentences = []\n",
    "with open(\"simple_sentences.txt\", 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "sentences = [x.strip() for x in sentences]\n",
    "\n",
    "dataloaders = []\n",
    "for i in range(1, max([len(x) for x in sentences])):\n",
    "    dataset = TextDataset(sentences, i)\n",
    "    dataloaders.append(DataLoader(dataset, batch_size))\n",
    "\n",
    "# Example training call\n",
    "train(model, dataloaders, vocab_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(sentence):\n",
    "    return torch.tensor([token_dict[x] for x in sentence], dtype=torch.int64, device=device)\n",
    "\n",
    "def run_inference(model, sentence):\n",
    "    input = vectorise(sentence)\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    logits = output[0][-1,:]\n",
    "\n",
    "    max_idx = logits.argmax().item()\n",
    "    for a, i in token_dict.items():\n",
    "        if max_idx == i:\n",
    "            sentence += a\n",
    "            return sentence\n",
    "    \n",
    "    print(max_idx)\n",
    "\n",
    "\n",
    "\n",
    "sentence = 'a dog run'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a dog runs\n",
      "a dog runs \n",
      "a dog runs o\n",
      "a dog runs on\n",
      "a dog runs on \n",
      "a dog runs on t\n",
      "a dog runs on th\n",
      "a dog runs on the\n",
      "a dog runs on the \n",
      "a dog runs on the b\n",
      "a dog runs on the be\n",
      "a dog runs on the bed\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    sentence = run_inference(model, sentence)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from json import JSONEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeTensor(JSONEncoder,Dataset):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().detach().numpy().tolist()\n",
    "        return super(json.NpEncoder, self).default(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = vectorise(sentence)\n",
    "y = model(x)[0]\n",
    "\n",
    "np.savetxt('test_data/generative_gru_x.csv', x.detach(), delimiter=',')\n",
    "np.savetxt('test_data/generative_gru_y.csv', y.detach(), delimiter=',')\n",
    "\n",
    "with open('models/generative_gru.json', 'w') as json_file:\n",
    "    json.dump(model.state_dict(), json_file,cls=EncodeTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
