{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gru_train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "with open(sentences_file, 'r') as f:\n",
    "    all_sentences = f.readlines()\n",
    "all_sentences = [x.replace('\\'', '') for x in all_sentences]\n",
    "all_sentences = [re.sub('\\W+', ' ', x.lower()).strip() for x in all_sentences]\n",
    "\n",
    "all_sentences = list(set(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(list(' '.join(all_sentences).split(' '))).value_counts().values)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing sentences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token set length:  28\n",
      "unique words:  526824\n",
      "word vocabulary length:  2829\n"
     ]
    }
   ],
   "source": [
    "sentences = compile_sentences(sentences_file, word_count_threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUTextGenerator(vocab_size, embed_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders, test_dataloaders = build_datasets(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example hyperparameters\n",
    "vocab_size = 28  # a-z and space, \\n chars\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "batch_size = 64\n",
    "\n",
    "# Create the model\n",
    "model = GRUTextGenerator(vocab_size, embed_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataloaders, test_dataloaders, vocab_size, device, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from model_cache/model_epoch_1.pt\n"
     ]
    }
   ],
   "source": [
    "load_model(model, 'model_cache/model_epoch_1.pt', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise(sentence):\n",
    "    return torch.tensor([token_dict[x] for x in sentence], dtype=torch.int64, device=device)\n",
    "\n",
    "def softmax(logits, temperature=1.0):\n",
    "    scaled_logits = logits / temperature\n",
    "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits))\n",
    "    probs = exp_logits / exp_logits.sum()\n",
    "    return probs\n",
    "\n",
    "def run_inference(model, sentence, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    input = vectorise(sentence)\n",
    "\n",
    "    output = model(input)\n",
    "\n",
    "    logits = output[0][-1, :].detach().numpy()\n",
    "\n",
    "    # print(logits)\n",
    "\n",
    "    # Compute probabilities using softmax with temperature\n",
    "    probs = softmax(logits, temperature)\n",
    "\n",
    "    # Randomly sample from the probability distribution\n",
    "    sampled_idx = np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    # sampled_idx = np.argmax(logits)\n",
    "    # print(sampled_idx)\n",
    "\n",
    "    # Find the corresponding token\n",
    "    for a, i in token_dict.items():\n",
    "        if sampled_idx == i:\n",
    "            sentence += a\n",
    "            return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeds\n"
     ]
    }
   ],
   "source": [
    "sentence = random.sample(string.ascii_lowercase, 1)[0]\n",
    "sentence = 'children'\n",
    "print(sentence, end='')\n",
    "\n",
    "while sentence[-1] != '\\n':\n",
    "    sentence = run_inference(model, sentence, temperature=0.6)\n",
    "    print(sentence[-1], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from json import JSONEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeTensor(JSONEncoder,Dataset):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().detach().numpy().tolist()\n",
    "        return super(json.NpEncoder, self).default(obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = vectorise(sentence)\n",
    "# y = model(x)[0]\n",
    "\n",
    "# np.savetxt('test_data/generative_gru_x.csv', x.detach(), delimiter=',')\n",
    "# np.savetxt('test_data/generative_gru_y.csv', y.detach(), delimiter=',')\n",
    "\n",
    "with open('models/generative_gru.json', 'w') as json_file:\n",
    "    json.dump(model.state_dict(), json_file,cls=EncodeTensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
